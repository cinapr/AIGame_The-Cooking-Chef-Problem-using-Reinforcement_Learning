{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6d3b61-1735-42e4-a502-425264ae8a7a",
   "metadata": {},
   "source": [
    "<center><h3 align='center'>Artificial Intelligence/Intelligent Agents 2022/2023</h3>\n",
    "<h4 align=\"center\">Homework 3 Reinforcement learning</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8270a9-e9d1-4ce5-bd8b-056292f0561c",
   "metadata": {},
   "source": [
    "Name : Cindy Aprilia<br>\n",
    "Matricole number : 286702"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62151ad9-1d12-4226-a9f4-ffcc74d8e34f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417a620e-d944-4d82-93c4-e882d5e58afe",
   "metadata": {},
   "source": [
    "The Cooking Chef Problem\n",
    "Consider the case where the agent is your personal Chef.\n",
    "In particular, the agent (the smiley on the map) wants to cook the eggs recipe according to your indication (scrambled or pudding).\n",
    "In order to cook the desired recipe, the agent must first collect the needed tools (the egg beater on the map). Then he must reach the stove (the frying pan or the oven on the map). Finally, he can cook.\n",
    "Not that there are two special interlinked cells (marked with the G) that allow the agent to go from one side of the map to the other. But to do so, the agent needs to express his will to go on the other side.\n",
    "Cells in (4, 1) and (6, 1) are the special gate ones. They allow the agent to go from one side of the map to another. Those two special cells are interlinked, but the agent needs to express his will to go on the other side (Left or Right).\n",
    "Since you have a lot hungry, it is fundamental that the agent cooks the eggs accordingly to your taste (scrambled/pudding) as fast as he can without letting you wait for more than necessary.\n",
    "\n",
    "In order to apply optimal control techniques such as value iteration, you need to model the aforementioned scenario as  an  MDP.  Recall  that  an  MDP  is  defined  as  tuple  (S, A, P, R, γ),  where: \n",
    "S: The (finite) set of all possible states.\n",
    "A: The (finite) set of all possible actions.\n",
    "\n",
    "P :  The transition function P : S x S x A [0, 1], which maps (sJ, s, a) to P (sJ|s, a), i.e., the probability of transitioning to state sJ ∈ S when taking action a ∈ A in state s ∈ S. \n",
    "Note that \n",
    "Σs’∈S P(S’|s,a) = 1 for all s ∈ S, a ∈ A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfd8b2-3a0e-4f6d-881c-f5a4ead4bbca",
   "metadata": {},
   "source": [
    "Figure 1: A particular instance of the cooking Chef problem. The goal is for the agent currently located in state (4, 3) to have a policy that always leads to cooking the eggs in location (4, 4) or (8, 4). Cells in (4, 1) and (6, 1) are the special gate ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7053bc-0da8-48a8-9ad1-826531a64db8",
   "metadata": {},
   "source": [
    "R:  The reward function R : S × A × S → R, which maps (s, a, sJ) to R(s, a, sJ), i.e., the reward obtained when taking action a ∈ A in state s ∈ S and arriving at state sJ ∈ S.\n",
    "γ: The discount factor, which controls how important are rewards in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f4555-77c3-48ae-810b-cc66c017de4d",
   "metadata": {},
   "source": [
    "To answer the questions, consider the instance shown in Figure 1:\n",
    "• In the figure, the agent is at (4; 3) (but it can start at any of the grid cells).\n",
    "• The agent needed cooking tools as the egg beater is in position (1; 3) and (8; 3).\n",
    "• There are two different final goals, displayed as the frying pan is in position (1; 4) and the oven in position (8; 4).\n",
    "• Cells in (4; 1) and (6; 1) are the special gate ones. They allow the agent to go from one side of the map to another. Those two special cells are interlinked, but the agent needs to express his will to go on the other side.\n",
    "• The agent is not able to move diagonally.\n",
    "• Walls are represented by thick black lines.\n",
    "• The agent cannot move through walls.\n",
    "• An episode will end when the agent successfully cooks the scrambled eggs (see the above description)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a84657-72e5-44ed-92e5-b07f06db8896",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1e157-fddd-498d-9ed5-d49cfccde47b",
   "metadata": {},
   "source": [
    "<b>Part a</b><br>\n",
    "Modeling the MDP as an infinite horizon MDP: the agent, once he starts to cook successfully, never ends, and it remains in an absorbing state.\n",
    "\n",
    "Using the above problem description, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103a19d-3445-4f6d-9406-29c41e41a5ce",
   "metadata": {},
   "source": [
    "<b><i>In the answers, I change the tupple position as (x,y), which the x-axis is the horizontal row, and the y-axis is the vertical column.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e610d-a604-454c-ae94-b96214fcf0fc",
   "metadata": {},
   "source": [
    "a) Provide a concise description of the states of the MDP. \n",
    "How many states are in this MDP? (i.e.what is |S|).\n",
    "\n",
    "<i>\n",
    "The amount of possible combinations of many factors would determine how many states there would be in the MDP. The combination factors usually consist of the environment (the map, the tools, the stove, and the recipe), and the stating point of the agent).\n",
    "    \n",
    "The states of the MDP in this example are represented by the grid cells of the kitchen. Each state in the question is a <b>tuple of the form (i, j) where i and j are the row and column number respectively</b>. These represent the coordinates of the agent's current position on the grid. \n",
    "Additionally, each state can be described by its attributes, such as whether the agent has collected the cooking tools, the position of the egg beater, the position of the stove, whether the agent is on one side or the other of the special gates and the recipe that the agent is trying to cook.\n",
    "    \n",
    "There are <b>36 total states</b> in this MDP (9 columns x 4 rows). \n",
    "There are 4 unaccessible cells in column 5, however this not decrease number of the state, as column 5 cells will be put between 2 walls with special door function.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cc9609-7742-420b-bff1-61657391ae5d",
   "metadata": {},
   "source": [
    "b) Provide a concise description of the actions of the MDP.\n",
    "How many actions are in this MDP? (i.e.what is |A|).\n",
    "\n",
    "<i>\n",
    "There are <b>4 actions</b> in this MDP. \n",
    "Each action corresponds to a movement of the agent on the grid, but there are some additional action.\n",
    "It is important to note that the <u>agent can only move in four cardinal direction</u> and can not move diagonally.\n",
    "    \n",
    "The actions of the MDP in this example include:\n",
    "<li>moving up</li>\n",
    "<li>moving down</li>\n",
    "<li>moving left</li>\n",
    "<li>moving right</li>\n",
    "<br>\n",
    "There are additional 'special' actions of the MDP in this example too, which includes:\n",
    "<li>using the special gates - right (i.e. expressing the will to go to the other side of the kitchen)</li>\n",
    "<li>using the special gates - left (i.e. expressing the will to go to the other side of the kitchen)</li>\n",
    "<li>Picking up tools</li>\n",
    "</i>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f89f892-4182-442a-8f00-2506bd3cc7f4",
   "metadata": {},
   "source": [
    "c) What is the dimensionality of the transition function P?\n",
    "\n",
    "<i>    \n",
    "The dimensionality of the transition function P is |S| x |A| x |S|.\n",
    "The number of possible states |S| is 36.\n",
    "The number of possible actions |A| is 4 which includes only common actions.\n",
    "So, the <b>dimensionality of P will be 36 x 4 x 36</b>.\n",
    "\n",
    "It assigns a probability to each possible transition given a current state and action.\n",
    "    \n",
    "However, the number may be a little lower than the real computation, due to the 'special' action and walls blockage.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b1c36-0427-417d-989b-cc1c38378d5d",
   "metadata": {},
   "source": [
    "d) Report the transition function P for any state s and action a in a tabular format.\n",
    "\n",
    "<i>\n",
    "It is not possible for me to report the transition function P for every state S and action A in a tabular format because it would require a huge amount of data, as it would be a 36 x 4 x 36 table.\n",
    "Therefore, I will give some examples of how the transition function P would work for a specific state and action.\n",
    "    \n",
    "<hr>\n",
    "\n",
    "For example, consider the state s = (3, 4) and the action a = \"move right\".\n",
    "\n",
    "If the agent attempts to move right from state (3, 4), and there is no wall or obstacle in the way, then the agent will transition to state (3, 5).<br>\n",
    "If there is a wall in the way, the agent will stay in the same state (3, 4).<br>\n",
    "If the agent is attempting to move diagonally, the agent will stay in the same state (3, 4).<br>\n",
    "<br>\n",
    "It's important to notice that the transition function P is stochastic, as the agent may have a probability of failure when attempting to use the special gates or move through walls.<br>\n",
    "<br>\n",
    "PS. Here is some examples of the transition functions. The rest of the transition functions can be checked on attached spreedsheets\n",
    "<br>\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>State</th>\n",
    "    <th>Up</th>\n",
    "    <th>New Position After Up</th>\n",
    "    <th>Down</th>\n",
    "    <th>New Position After Down</th>\n",
    "    <th>Left</th>\n",
    "    <th>New Position After Left</th>\n",
    "    <th>Right</th>\n",
    "    <th>New Position After Right</th>\n",
    "    <th>Special Door Right</th>\n",
    "    <th>New Position After Special Door Right</th>\n",
    "    <th>Special Door Left</th>\n",
    "    <th>New Position After Special Door Left</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(1,1)</td>\n",
    "    <td>0 (Try to go up but there are wall, so stay in same position)</td>\n",
    "    <td>(1,1)</td>\n",
    "    <td>0 (Try to go down but there are wall, so stay in same position)</td>\n",
    "    <td>(1,1)</td>\n",
    "    <td>0 (Try to go left but there are wall, so stay in same position)</td>\n",
    "    <td>(1,1)</td>\n",
    "    <td>1 (Success go right)</td>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0 (No door)</td>\n",
    "    <td>(1,1)</td>\n",
    "    <td>0 (No door)</td>\n",
    "    <td>(1,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0.5</td>\n",
    "    <td>(1,1)</td>\n",
    "    <td>0.5</td>\n",
    "    <td>(3,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(2,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(3,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(3,2)</td>\n",
    "    <td>0</td>\n",
    "    <td>(3,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(2,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(4,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(3,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(3,1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>(4,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(4,2)</td>\n",
    "    <td>0</td>\n",
    "    <td>(4,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(3,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(4,1)</td>\n",
    "    <td>0.33</td>\n",
    "    <td>(6,1)</td>\n",
    "    <td>0</td>\n",
    "    <td>(4,1)</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b312f4-30ec-4fab-b4a5-a6d4a59a9f51",
   "metadata": {},
   "source": [
    "e) Describe a reward function R : S × A × S and a value of γ that will lead to an optimal policy\n",
    "\n",
    "<i>\n",
    "The <b>reward function assigns positive rewards for achieving the final goal of cooking the eggs in the desired recipe</b>, negative rewards for failing to use the special gates, negative rewards for each step taken before reaching the final goal and negative rewards for crashing into walls or attempting to move diagonally.<br>\n",
    "<br>\n",
    "A reward function R : S × A × S that will lead to an optimal policy could be defined as follows:<br>\n",
    "R(s, a, s') = -10 for crashing into a wall<br>\n",
    "R(s, a, s') = -1 for backtracking.<br>\n",
    "R(s, a, s') = 100 for successfully picking up a egg beater (only for the first time in each round, so the agent would not go back and forth in front of egg beater to accumulate reward).<br>\n",
    "R(s, a, s') = 100 for successfully cooking the eggs (only after picking up an egg beater, so the agent would not rush to cooking before using the egg beater).<br>\n",
    "R(s, a, s') = -1 for each step taken before the agent reaches the final goal.<br>\n",
    "\n",
    "<br>\n",
    "As for the discount factor, a value of γ that will lead to an optimal policy could be 0.9. \n",
    "A value of γ close to 1 will give more importance to future rewards while a value close to 0 will give more importance to immediate rewards. \n",
    "In this case, a value of 0.9 strikes a good balance between the two. \n",
    "<b>γ = 0.9</b> gives more weight to future rewards which means that the agent will try to reach the goal in fewer steps, but still take into account the immediate rewards, and it will not ignore them.\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc048cf-1cd7-40ac-87b2-322473bf6bf8",
   "metadata": {},
   "source": [
    "f) Does γ ∈ (0, 1) affect the optimal policy in this case? Explain why.\n",
    "\n",
    "<i>\n",
    "The discount factor (γ), determines the importance of future rewards. \n",
    "A value of γ close to 1 will place a high importance on future rewards, while a value close to 0 will place a low importance on future rewards.\n",
    "\n",
    "In the case of the Cooking Chef problem, <b>the optimal policy may be affected by the value of γ chosen</b>. For example, if γ is set to a high value, the agent may prioritize reaching the goal state (the frying pan or the oven) as quickly as possible, even if it means not collecting the cooking tools or going through the special gate cells. On the other hand, if γ is set to a lower value, the agent may prioritize collecting the cooking tools and going through the special gate cells, even if it takes longer to reach the goal state.\n",
    "\n",
    "So, <b>γ = 0.9</b> will strikes a good balance between the two. \n",
    "γ = 0.9 gives more weight to future rewards which means that the agent will try to reach the goal in fewer steps, but still take into account the immediate rewards, and it will not ignore them.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9c3de8-b6a3-41f7-bda6-77a375f206cd",
   "metadata": {},
   "source": [
    "g) How many possible policies are there? ( All policies, not just optimal policies.)\n",
    "\n",
    "<i>\n",
    "The number of possible policies in this case would depend on the number of states and actions available to the agent. Since the agent can start in any of the grid cells, and can move in four directions (up, down, left, right), there are potentially a large number of possible policies. Additionally, if there are multiple tools and stoves in different locations, the number of possible policies increases.\n",
    "    \n",
    "The rough calculation will be 4^36 possible policies, as there are 36 states and 4 'basic' actions in this MDP. However the exact number may be different due to special gates and walls blokages\n",
    "\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07f1e09-cf27-4f65-b1a2-160be19f17d9",
   "metadata": {},
   "source": [
    "h) Now, considering the problem as a model-free scenario, provide a program (written in python) able to compute the optimal policy for this world considering the pudding eggs scenario solely. Draw the computed policy in the grid by putting in each cell the optimal action. If multiple actions are possible, include the probability of each arrow. There may be multiple optimal policies, pick one to show it. Note that the model is not available for computation but must be encoded to be used as the \"real-world\" environment.\n",
    "\n",
    "<i>\n",
    "The code will use tupple position as (x,y), which the x-axis is the horizontal row, and the y-axis is the vertical column.\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a52bbe7d-b901-43cf-8621-29efb9b6a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab3e0bd-d050-45dc-88f3-9c0b234ea0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid size\n",
    "grid_size = (5, 10) #Because 0,0 still exist in array, but will be considered as unaccessible\n",
    "\n",
    "# Define the starting position of the agent\n",
    "starting_state = (3, 4)\n",
    "\n",
    "# Define the positions of the cooking tools (egg beater), frying pan and oven\n",
    "egg_beater_pos = [(3, 1), (3, 8)]\n",
    "frying_pan_pos = (4, 1)\n",
    "oven_pos = (4, 8)\n",
    "\n",
    "# Define the positions of the special gates\n",
    "gate_pos = [(1, 4), (1, 6)]\n",
    "\n",
    "# Define the actions the agent can take (up, down, left, right)\n",
    "possible_actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "# Define the learning rate and discount factor\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "\n",
    "#Randomize of the way taken heuristic moves\n",
    "epsilon = 1\n",
    "isRandom = True\n",
    "\n",
    "#How many testing\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5222bd94-81e6-4e56-8fc5-f8df5c18c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the reward \n",
    "def get_reward(current_state, next_state, num_steps, prev_state, fulfiled_prev):\n",
    "    reward = 0\n",
    "    if check_walls(current_state, next_state):\n",
    "        reward = -10\n",
    "    if ((current_state == prev_state) and (prev_state != (0,0))): #Not new, but backtracking\n",
    "        reward = -1\n",
    "    if ((next_state == prev_state) and (prev_state != (0,0))): #Not new, but backtracking\n",
    "        reward = -1\n",
    "    if current_state == next_state: #Not new, but backtracking\n",
    "        reward = -1\n",
    "    if ((fulfiled_prev == False) and (next_state in egg_beater_pos)):\n",
    "        reward = 100 - num_steps\n",
    "    if ((fulfiled_prev) and (next_state == frying_pan_pos or next_state == oven_pos)):\n",
    "        reward = 100 - num_steps\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cd87a71-30e6-4696-8524-f82f4777ad84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check Walls :\n",
    "def check_walls(current_state, next_state):\n",
    "    #From/To (1,1) to/from (2,1)\n",
    "    if ((current_state == (1,1)) and (next_state == (2,1))):\n",
    "        return True\n",
    "    elif ((current_state == (2,1)) and (next_state == (1,1))):\n",
    "        return True\n",
    "\n",
    "    #From/To (1,2) to/from (2,2)\n",
    "    elif ((current_state == (1,2)) and (next_state == (2,2))):\n",
    "        return True\n",
    "    elif ((current_state == (2,2)) and (next_state == (1,2))):\n",
    "        return True\n",
    "\n",
    "    #From/To (2,2) to/from (3,2)\n",
    "    elif ((current_state == (2,2)) and (next_state == (3,2))):\n",
    "        return True\n",
    "    elif ((current_state == (3,2)) and (next_state == (2,2))):\n",
    "        return True\n",
    "\n",
    "    #From/To (2,3) to/from (3,3)\n",
    "    elif ((current_state == (2,3)) and (next_state == (3,3))):\n",
    "        return True\n",
    "    elif ((current_state == (3,3)) and (next_state == (2,3))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,1) to/from (4,1)\n",
    "    elif ((current_state == (3,1)) and (next_state == (4,1))):\n",
    "        return True\n",
    "    elif ((current_state == (4,1)) and (next_state == (3,1))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,1) to/from (3,2)\n",
    "    elif ((current_state == (3,1)) and (next_state == (3,2))):\n",
    "        return True\n",
    "    elif ((current_state == (3,2)) and (next_state == (3,1))):\n",
    "        return True\n",
    "\n",
    "    #From/To (1,8) to/from (2,8)\n",
    "    elif ((current_state == (1,8)) and (next_state == (2,8))):\n",
    "        return True\n",
    "    elif ((current_state == (2,8)) and (next_state == (1,8))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,8) to/from (4,8)\n",
    "    elif ((current_state == (3,8)) and (next_state == (4,8))):\n",
    "        return True\n",
    "    elif ((current_state == (4,8)) and (next_state == (3,8))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,9) to/from (4,9)\n",
    "    elif ((current_state == (3,9)) and (next_state == (4,9))):\n",
    "        return True\n",
    "    elif ((current_state == (4,9)) and (next_state == (3,9))):\n",
    "        return True\n",
    "\n",
    "    #From/To (2,7) to/from (2,8)\n",
    "    elif ((current_state == (2,7)) and (next_state == (2,8))):\n",
    "        return True\n",
    "    elif ((current_state == (2,8)) and (next_state == (2,7))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,7) to/from (3,8)\n",
    "    elif ((current_state == (3,7)) and (next_state == (3,8))):\n",
    "        return True\n",
    "    elif ((current_state == (3,8)) and (next_state == (3,7))):\n",
    "        return True\n",
    "\n",
    "    #From/To (1,4) to/from (1,5)\n",
    "    elif ((current_state == (1,4)) and (next_state == (1,5))):\n",
    "        return True\n",
    "    elif ((current_state == (1,5)) and (next_state == (1,4))):\n",
    "        return True\n",
    "\n",
    "    #From/To (1,5) to/from (1,6)\n",
    "    elif ((current_state == (1,5)) and (next_state == (1,6))):\n",
    "        return True\n",
    "    elif ((current_state == (1,6)) and (next_state == (1,5))):\n",
    "        return True\n",
    "\n",
    "    #From/To (2,4) to/from (2,5)\n",
    "    elif ((current_state == (2,4)) and (next_state == (2,5))):\n",
    "        return True\n",
    "    elif ((current_state == (2,5)) and (next_state == (2,4))):\n",
    "        return True\n",
    "\n",
    "    #From/To (2,5) to/from (2,6)\n",
    "    elif ((current_state == (2,5)) and (next_state == (2,6))):\n",
    "        return True\n",
    "    elif ((current_state == (2,6)) and (next_state == (2,5))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,4) to/from (3,5)\n",
    "    elif ((current_state == (3,4)) and (next_state == (3,5))):\n",
    "        return True\n",
    "    elif ((current_state == (3,5)) and (next_state == (3,4))):\n",
    "        return True\n",
    "\n",
    "    #From/To (3,5) to/from (3,6)\n",
    "    elif ((current_state == (3,5)) and (next_state == (3,6))):\n",
    "        return True\n",
    "    elif ((current_state == (3,6)) and (next_state == (3,5))):\n",
    "        return \n",
    "\n",
    "    #From/To (4,4) to/from (4,5)\n",
    "    elif ((current_state == (4,4)) and (next_state == (4,5))):\n",
    "        return True\n",
    "    elif ((current_state == (4,5)) and (next_state == (4,4))):\n",
    "        return True\n",
    "\n",
    "    #From/To (4,5) to/from (4,6)\n",
    "    elif ((current_state == (4,5)) and (next_state == (4,6))):\n",
    "        return True\n",
    "    elif ((current_state == (4,6)) and (next_state == (4,5))):\n",
    "        return True\n",
    "\n",
    "    else :\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ed0ff24-cf61-4d79-9357-cda9b673bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Grid Boundaries\n",
    "def OutOfBoundaries(next_state, grid_size):\n",
    "    if ((next_state[0] < 1) or (next_state[0] >= grid_size[0]) or (next_state[1] < 1) or (next_state[1] >= grid_size[1]) ):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b45694-22a8-4f4f-ab82-1aad0b381517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moving Due to Special Gate\n",
    "def MoveSpecialGate(next_state, gate_pos):\n",
    "    if next_state in gate_pos:\n",
    "        if next_state == gate_pos[0]:\n",
    "            return gate_pos[1], True\n",
    "        elif next_state == gate_pos[1]:\n",
    "            return gate_pos[0], True\n",
    "\n",
    "    return next_state, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf5269-97f5-400a-bb79-0d1b55d022d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check finish game yet\n",
    "def check_terminal_state(current_state, pass_requirement):\n",
    "    #Always need to pass egg_beater first\n",
    "    if (pass_requirement == False):\n",
    "        return False\n",
    "\n",
    "    terminal_states = [frying_pan_pos, oven_pos]\n",
    "    if current_state in terminal_states:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "438860c7-2861-45c1-9aa2-538970df3446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a function to choose the next action based on the Q-table\n",
    "def choose_action(current_state, prev_state, q_table, possible_actions, epsilon, isRandom, final_pos):\n",
    "    #if epsilon high therefore learning, or when there are no learning history\n",
    "    if (np.sum(q_table) == 0) or (np.random.uniform(0, 1) < epsilon):\n",
    "        if (isRandom == True):\n",
    "            #either take from random\n",
    "            action = np.random.choice(possible_actions)\n",
    "        else :\n",
    "            #or use auclean to nearer\n",
    "            action = move_towards_target(current_state, prev_state, final_pos, possible_actions)\n",
    "    #if Epsilon low therefore not learning and get from q_table\n",
    "    else:\n",
    "        state_action = q_table[current_state[0], current_state[1]]\n",
    "        action = possible_actions[np.argmax(state_action)]\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd59928d-a024-47ce-ae02-f4636d8a7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(start, goal):\n",
    "    queue = deque([start])\n",
    "    visited = set([start])\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        \n",
    "        if current == goal:\n",
    "            return True  # goal found\n",
    "        \n",
    "        for next_state in get_valid_states(current):\n",
    "            if next_state not in visited:\n",
    "                visited.add(next_state)\n",
    "                queue.append(next_state)\n",
    "    \n",
    "    return False  # goal not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ebde9fd-83d3-4f68-8a37-66b572997b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_states(current_state):\n",
    "    valid_states = []\n",
    "    for action in actions:\n",
    "        next_state = get_next_step(action, current_state)\n",
    "        if next_state and not check_walls(current_state, next_state):\n",
    "            valid_states.append(next_state)\n",
    "    return valid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99a0d68-c345-496a-865b-63165e6c1fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Decide which beater to be taken\n",
    "def move_towards_beater(current_state, egg_beater_pos):\n",
    "    egg_beater_no = 0\n",
    "    egg_beater_distance = (grid_size[0] * grid_size[1])\n",
    "\n",
    "    for egg_beater_no_iter in range(len(egg_beater_pos)):\n",
    "        distance = np.linalg.norm(np.array(current_state) - np.array(egg_beater_pos[egg_beater_no_iter]))\n",
    "        if (distance < egg_beater_distance):\n",
    "            egg_beater_no = egg_beater_no_iter\n",
    "            egg_beater_distance = distance\n",
    "\n",
    "    return egg_beater_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f170b7e2-fb50-475a-86cf-e133674ef61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Push the action to closer to the oven/frying pan/beater -> final_pos = oven_pos for oven and frying_pan_pos for frying pan\n",
    "def move_towards_target(current_state, prev_state, final_pos, possible_actions):\n",
    "    best_action = None\n",
    "    best_distance = float(\"inf\")\n",
    "\n",
    "    for action in possible_actions:\n",
    "        if action == \"up\":\n",
    "            next_state = (current_state[0]-1, current_state[1])\n",
    "        elif action == \"down\":\n",
    "            next_state = (current_state[0]+1, current_state[1])\n",
    "        elif action == \"left\":\n",
    "            next_state = (current_state[0], current_state[1]-1)\n",
    "        elif action == \"right\":\n",
    "            next_state = (current_state[0], current_state[1]+1)\n",
    "\n",
    "        if ((check_walls(current_state, next_state) == False) and (bfs(next_state, final_pos))):\n",
    "            distance = np.sqrt((next_state[0] - final_pos[0])**2 + (next_state[1] - final_pos[1])**2)\n",
    "            if (prev_state == next_state):\n",
    "                continue\n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_action = action\n",
    "\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f788ca5-cbec-44d7-a4e1-d84317f06f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get Next Step of Action :\n",
    "def get_next_step(action, current_state):\n",
    "    next_state = None\n",
    "\n",
    "    if action == \"up\": # up\n",
    "        next_state = (current_state[0]-1, current_state[1])\n",
    "    elif action == \"down\": # down\n",
    "        next_state = (current_state[0]+1, current_state[1])\n",
    "    elif action == \"left\": # left\n",
    "        next_state = (current_state[0], current_state[1]-1)\n",
    "    elif action == \"right\": # right\n",
    "        next_state = (current_state[0], current_state[1]+1)\n",
    "\n",
    "    return next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "203c3fcd-3cb2-482d-8b61-0234b6921682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Move agent\n",
    "def transition_function(current_state, prev_state, action, num_steps, fulfiled_prev, is_terminal):\n",
    "    next_state = None\n",
    "    reward = None\n",
    "    #is_terminal = None\n",
    "    #fulfiled_prev = True\n",
    "\n",
    "    next_state = get_next_step(action, current_state)\n",
    "\n",
    "    # Check if the next state is a wall\n",
    "    if check_walls(current_state, next_state):\n",
    "        #print(\"Hit the wall | \" + str(num_steps) + \" | prev_state = \" + str(prev_state) + \" current_state = \" + str(current_state) + \" next_state = \" + str(next_state))\n",
    "        if (prev_state == (0,0)):\n",
    "            next_state = current_state #For the 1st time if the next state hit walls therefore you won't move instead of backtrack\n",
    "        else :\n",
    "            next_state = prev_state\n",
    "    \n",
    "    #Skip if out of boundaries\n",
    "    if OutOfBoundaries(next_state, grid_size):\n",
    "        #print(\"Out of Boundaries | \" + str(num_steps) + \" | prev_state = \" + str(prev_state) + \" current_state = \" + str(current_state) + \" next_state = \" + str(next_state))\n",
    "        if (prev_state == (0,0)):\n",
    "            next_state = current_state #For the 1st time if the next state hit walls therefore you won't move instead of backtrack\n",
    "        else :\n",
    "            next_state = prev_state\n",
    "\n",
    "    # Check if the next state is a special gate and update the state accordingly\n",
    "    next_state, useSpecialGate = MoveSpecialGate(next_state, gate_pos)\n",
    "\n",
    "    # Check if pick up egg_beater\n",
    "    if (next_state in egg_beater_pos):\n",
    "        #print(\"EGG BEATER FOUND | \" + str(num_steps) + \" | prev_state = \" + str(prev_state) + \" current_state = \" + str(current_state) + \" next_state = \" + str(next_state))\n",
    "        fulfiled_prev = True\n",
    "\n",
    "    #check the game is final yet\n",
    "    is_terminal = check_terminal_state(next_state, fulfiled_prev)\n",
    "\n",
    "    # Get the reward for the current state, action and next state\n",
    "    reward = get_reward(current_state, next_state, num_steps, prev_state, fulfiled_prev)\n",
    "\n",
    "    return next_state, reward, is_terminal, fulfiled_prev, useSpecialGate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d32fbe8f-d433-452d-98c6-01f2898d703f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finish in : 44 steps.\n",
      "[(3, 4), (4, 4), (3, 4), (4, 4), (3, 4), (2, 4), (3, 4), (4, 4), (3, 4), (2, 4), 'UseGate', (1, 6), (2, 4), (2, 3), (2, 2), (2, 1), (2, 2), (2, 1), (3, 1), (2, 1), (3, 1), (2, 1), (3, 1), (2, 1), (3, 1), (2, 1), (2, 2), (2, 3), (2, 2), (2, 3), (2, 4), (3, 4), (3, 3), (3, 4), (3, 3), (3, 2), (3, 3), (3, 4), (3, 3), (3, 2), (4, 2), (3, 2), (4, 2), (3, 2), (4, 2)]\n",
      "\n",
      "Finish in : 49 steps.\n",
      "[(3, 4), (2, 4), (2, 3), (2, 4), (2, 3), (2, 4), (2, 3), (2, 4), (3, 4), (4, 4), (3, 4), (4, 4), (3, 4), (4, 4), (4, 3), (4, 4), (4, 3), (4, 4), (3, 4), (2, 4), 'UseGate', (1, 6), (2, 4), 'UseGate', (1, 4), (2, 4), (2, 3), (2, 2), (2, 3), (2, 2), (2, 1), (3, 1), (2, 1), (3, 1), (2, 1), (2, 2), (2, 3), (2, 4), (3, 4), (3, 3), (4, 3), (4, 4), (4, 3), (4, 2), (4, 3), (4, 4), (4, 3), (4, 2), (4, 3), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "# Load the Q-table if it exists\n",
    "try:\n",
    "    q_table = np.load(\"q_table.npy\")\n",
    "except:\n",
    "    q_table = np.zeros(grid_size)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    savePathArray = [] #Save path to be printed in the end\n",
    "    \n",
    "    #define starting value\n",
    "    current_state = starting_state\n",
    "    num_of_steps = 0\n",
    "    prev_state = (0,0)\n",
    "    \n",
    "    #Define starting check toggle\n",
    "    is_terminal = False #isFoundStove\n",
    "    fulfiled_prev = False #isFoundBeater\n",
    "\n",
    "    #get egg beater that be targeted (nearest egg beater with euclian distance)\n",
    "    egg_beater_target = egg_beater_pos[move_towards_beater(current_state, egg_beater_pos)]\n",
    "    \n",
    "    while not is_terminal:\n",
    "        final_pos = egg_beater_target if (fulfiled_prev == False) else frying_pan_pos #oven_pos \n",
    "\n",
    "        # Choose an action based on the current state\n",
    "        action = choose_action(current_state, prev_state, q_table, possible_actions, epsilon, isRandom, final_pos)\n",
    "\n",
    "        # Perform the action and observe the next state and reward\n",
    "        next_state, reward, is_terminal, fulfiled_prev, useSpecialGate = transition_function(current_state, prev_state, action, num_of_steps, fulfiled_prev, is_terminal)\n",
    "        \n",
    "        # Update the Q-value for the current state and action\n",
    "        next_row, next_col = next_state\n",
    "        q_table[current_state[0], current_state[1]] = (1 - alpha) * q_table[current_state[0], current_state[1]] + alpha * (reward + gamma * np.max(q_table[next_row, next_col]))\n",
    "\n",
    "        savePathArray.append(current_state)\n",
    "        \n",
    "        prev_state = current_state\n",
    "        current_state = next_state\n",
    "        \n",
    "        if (is_terminal):\n",
    "            # Save the Q-table after training\n",
    "            print(\"\\nFinish in : \" + str(num_of_steps) + \" steps.\\n\" + str(savePathArray))\n",
    "            np.save(\"q_table.npy\", q_table)\n",
    "\n",
    "        if (num_of_steps > 50):\n",
    "            #print(\"BREAK EPISODE : \" + str(episode) + \" to prevent infinite loop | STEPS=\" + str(num_of_steps) + \" | prev_state = \" + str(prev_state) + \" current_state = \" + str(current_state) + \" next_state = \" + str(next_state))\n",
    "            break\n",
    "        else :\n",
    "            #print(\"EPISODE : \" + str(episode) + \" | STEPS=\" + str(num_of_steps) + \" | TARGET=\" + str(final_pos) + \" | prev_state = \" + str(prev_state) + \" current_state = \" + str(current_state) + \" next_state = \" + str(next_state))\n",
    "            pass\n",
    "        \n",
    "        num_of_steps += 1\n",
    "        \n",
    "        #If using special gate the steps was added 1 for declaring the usage of special gate\n",
    "        if (useSpecialGate):\n",
    "            num_of_steps += 1\n",
    "            savePathArray.append('UseGate')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db89eef-6e15-4d3f-a986-8297e42312d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4992f015-cfff-4683-8832-f3e16ca9ec8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "i) Is the computed policy deterministic or stochastic?\n",
    "\n",
    "<i>\n",
    "The computed policy is deterministic if the agent always takes the same action in the same state, based on the information available to it (agent always chooses the action with the highest expected value).\n",
    "\n",
    "However, if we use epsilon-greedy exploration strategy (agent will choose a random action with probability epsilon). This means that there is a chance that the agent will choose a different action than the one with the highest value in the Q-table, making the policy non-deterministic.\n",
    "\n",
    "In other words, <b>The computed policy is stochastic</b> because there is a probability that the agent will choose a random action rather than the action with the highest value in the Q-table. This <b>allows the agent to explore the action space and improve its knowledge of the environment</b>.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce296c-ea22-469b-9b47-8bb18b068627",
   "metadata": {},
   "source": [
    "j) Is there any advantage to having a stochastic policy? Explain.\n",
    "\n",
    "<i>\n",
    "Yes, there are advantages to having a stochastic policy.\n",
    "\n",
    "Some of stochastic policy examples :\n",
    "<li>Allows the agent to explore the action space and improve its knowledge of the environment.</li>\n",
    "<li>Agent with a stochastic policy is more likely to explore different actions, even if they have not been previously selected, this means that it will not get stuck in a local optimum and can learn the optimal policy for the environment.</li>\n",
    "<li>Can help to avoid getting stuck in suboptimal solutions. In some cases, there may be multiple optimal solutions, and a deterministic policy may only find one of them. A stochastic policy allows the agent to explore different solutions, increasing the chances of finding the global optimum.</li>\n",
    "<li>A stochastic policy allows the agent to adapt to changes in the environment and uncertainty, making it more robust. Because, in the real-world problems, the environment is not deterministic, and the agent cannot rely on a strict deterministic policy.</li>\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4479832b-9a2d-4e17-b4cc-1e3d49c39662",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16139945-6b6b-4daa-925d-e7204dcb5be4",
   "metadata": {},
   "source": [
    "<b>Part b</b>\n",
    "\n",
    "Now consider that your agent, because of his tiredness, might goes in the wrong direction. Then each action has a 60% chance of going in the chosen direction and 40% chance of going perpendicular to the right of the direction chosen. Accordingly, with these new settings, answer the following\n",
    "questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b441248d-cfc3-40b8-a4a2-024f8d930953",
   "metadata": {},
   "source": [
    "a) Report the transition function P for any state s and action a ∈ A.\n",
    "\n",
    "<i>\n",
    "The transition function P for any state s and action a ∈ A in this scenario would be as follows:\n",
    "\n",
    "<li>For any action a that moves the agent in the intended direction, there is a 60% chance that the agent will end up in the intended state sJ and a 40% chance that the agent will end up in a state that is perpendicular to the right of the intended direction.<br>\n",
    "For any action a that would result in the agent hitting a wall, the agent will remain in the same state S.</li>\n",
    "<li>For any action a that would result in the agent reaching the special gate cells (4,1) or (6,1), there is a 100% chance that the agent will transition to the corresponding special gate cell on the opposite side of the map.</li>\n",
    "<li>For any action a that would result in the agent reaching the egg beater or the stove, there is a 100% chance that the agent will transition to the state where the egg beater or stove is located and pick it up.</li>\n",
    "So for any state s and action a, the transition function P would be represented as a probability distribution over the possible next states sJ, with the probabilities determined by the conditions above.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc99fb57-4802-4914-8833-e90311b092aa",
   "metadata": {},
   "source": [
    "b) Does the optimal policy change compared to Part a? Justify your answer.\n",
    "\n",
    "<i>\n",
    "With this new policy, the agent's movements will be less predictable and there is a higher chance of the agent deviating from its intended direction. This will make it more difficult for the agent to reach its goal and make the cooking process longer. The optimal policy will have to take this into account and may require more steps or a more complex strategy to reach the final goal. \n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b917f2-8125-4f7f-af9b-f9d80ffc4436",
   "metadata": {},
   "source": [
    "c) Will the value of the optimal policy change? Explain how.\n",
    "\n",
    "<i>\n",
    "The value of the optimal policy may also change as the agent will likely receive lower rewards due to the increased chance of deviating from the intended path. The agent may also have a higher chance of getting stuck in certain states as it deviates from its intended direction, which will also affect the value of the optimal policy. Overall, the new policy makes the problem more challenging and requires a more robust solution to find the optimal policy.\n",
    "\n",
    "However, it's also possible that the new policy allows the agent to explore more states and therefore find a better optimal policy with higher value. The optimal policy may also change as the agent may now have a higher chance of ending up in a state that was not previously considered optimal due to the increased probability of deviation.\n",
    "</i>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
